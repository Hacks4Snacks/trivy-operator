package controller

import (
	"context"
	"crypto/sha256"
	"encoding/json"
	"errors"
	"fmt"
	"net/http"
	"os"
	"path/filepath"
	"strings"
	"time"

	"github.com/go-logr/logr"
	"go.uber.org/multierr"
	batchv1 "k8s.io/api/batch/v1"
	k8sapierror "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/builder"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/predicate"
	"sigs.k8s.io/controller-runtime/pkg/reconcile"

	"github.com/aquasecurity/trivy-operator/pkg/apis/aquasecurity/v1alpha1"
	"github.com/aquasecurity/trivy-operator/pkg/exposedsecretreport"
	"github.com/aquasecurity/trivy-operator/pkg/kube"
	"github.com/aquasecurity/trivy-operator/pkg/operator/etc"
	"github.com/aquasecurity/trivy-operator/pkg/sbomreport"
	"github.com/aquasecurity/trivy-operator/pkg/trivyoperator"
	"github.com/aquasecurity/trivy-operator/pkg/vulnerabilityreport"

	. "github.com/aquasecurity/trivy-operator/pkg/operator/predicate"
)

// ScanJobController watches Kubernetes workloads and generates
// v1alpha1.VulnerabilityReport instances using vulnerability scanner that that
// implements the Plugin interface.
type ScanJobController struct {
	logr.Logger
	etc.Config
	kube.ObjectResolver
	kube.LogsReader
	vulnerabilityreport.Plugin
	trivyoperator.PluginContext
	trivyoperator.ConfigData
	SbomReadWriter          sbomreport.ReadWriter
	VulnerabilityReadWriter vulnerabilityreport.ReadWriter
	ExposedSecretReadWriter exposedsecretreport.ReadWriter
}

// Manage scan jobs with image pull secrets
// kubebuilder:rbac:groups="",resources=secrets,verbs=create;update
// +kubebuilder:rbac:groups=batch,resources=jobs,verbs=get;list;watch;create;delete

func (r *ScanJobController) SetupWithManager(mgr ctrl.Manager) error {
	var predicates []predicate.Predicate
	if !r.ConfigData.VulnerabilityScanJobsInSameNamespace() {
		predicates = append(predicates, InNamespace(r.Config.Namespace))
	}
	predicates = append(predicates, ManagedByTrivyOperator, IsVulnerabilityReportScan, JobHasAnyCondition)
	return ctrl.NewControllerManagedBy(mgr).
		Named("scan-job-controller").
		For(&batchv1.Job{}, builder.WithPredicates(predicates...)).
		Complete(r.reconcileJobs())
}

func (r *ScanJobController) reconcileJobs() reconcile.Func {
	return func(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
		log := r.Logger.WithValues("job", req.NamespacedName)

		// Check if Trivy server is ready in client-server mode
		// Infer client-server mode by checking if a Trivy server URL is set
		if serverEndpoint := r.ConfigData.GeTrivyServerURL(); serverEndpoint != "" {
			if !r.isTrivyServerReady(ctx) {
				log.V(1).Info("Trivy server is not ready, requeuing job")
				return ctrl.Result{RequeueAfter: 10 * time.Second}, nil
			}
		}

		job := &batchv1.Job{}
		err := r.Client.Get(ctx, req.NamespacedName, job)
		if err != nil {
			if k8sapierror.IsNotFound(err) {
				log.V(1).Info("Ignoring cached job that must have been deleted")
				return ctrl.Result{}, nil
			}
			return ctrl.Result{}, fmt.Errorf("getting job from cache: %w", err)
		}

		if len(job.Status.Conditions) == 0 {
			log.V(1).Info("Ignoring Job without conditions")
			return ctrl.Result{}, nil
		}

		switch jobCondition := job.Status.Conditions[0].Type; jobCondition {
		case batchv1.JobComplete, batchv1.JobSuccessCriteriaMet, batchv1.JobFailed, batchv1.JobFailureTarget:
			completedContainers, err := r.completedContainers(ctx, job)
			if err != nil {
				return ctrl.Result{}, r.deleteJob(ctx, job)
			}
			if len(completedContainers) == 0 {
				return ctrl.Result{}, r.deleteJob(ctx, job)
			}
			return ctrl.Result{}, r.processCompleteScanJob(ctx, job, completedContainers...)

		default:
			return ctrl.Result{}, fmt.Errorf("unrecognized scan job condition: %v", jobCondition)
		}
	}
}

 // isTrivyServerReady checks if the Trivy server is ready to process scans by querying its health endpoint.
func (r *ScanJobController) isTrivyServerReady(ctx context.Context) bool {
	// Get the Trivy server endpoint from configuration
	serverEndpoint := r.ConfigData.GeTrivyServerURL()
	if serverEndpoint == "" {
		r.Logger.V(1).Info("Trivy server endpoint not configured, assuming not ready")
		return false
	}

	// Construct the health endpoint URL
	healthURL := fmt.Sprintf("%s/health", serverEndpoint)
	if !strings.HasPrefix(healthURL, "http://") && !strings.HasPrefix(healthURL, "https://") {
		healthURL = fmt.Sprintf("http://%s", healthURL)
	}

	// Create an HTTP client with a timeout
	client := &http.Client{
		Timeout: 5 * time.Second,
	}

	// Make a request to the health endpoint
	req, err := http.NewRequestWithContext(ctx, "GET", healthURL, nil)
	if err != nil {
		r.Logger.Error(err, "Failed to create request for Trivy server health check", "url", healthURL)
		return false
	}

	resp, err := client.Do(req)
	if err != nil {
		r.Logger.Error(err, "Failed to check Trivy server health", "url", healthURL)
		return false
	}
	defer resp.Body.Close()

	// Check if the status code indicates the server is healthy (e.g., 200 OK)
	if resp.StatusCode == http.StatusOK {
		r.Logger.V(1).Info("Trivy server is ready", "url", healthURL)
		return true
	}

	r.Logger.V(1).Info("Trivy server not ready", "url", healthURL, "status", resp.StatusCode)
	return false
}

func (r *ScanJobController) processCompleteScanJob(ctx context.Context, job *batchv1.Job, completedContainers ...string) error {
	log := r.Logger.WithValues("job", fmt.Sprintf("%s/%s", job.Namespace, job.Name))
	log.V(1).Info("Processing completed scan job", "completedContainers", completedContainers)

	ownerRef, err := kube.ObjectRefFromObjectMeta(job.ObjectMeta)
	if err != nil {
		return fmt.Errorf("getting owner ref from scan job metadata: %w", err)
	}

	owner, err := r.ObjectFromObjectRef(ctx, ownerRef)
	if err != nil {
		if k8sapierror.IsNotFound(err) {
			log.V(1).Info("Report owner must have been deleted", "owner", owner)
			return r.deleteJob(ctx, job)
		}
		return fmt.Errorf("getting object from object ref: %w", err)
	}
	podSpecHash, ok := job.Labels[trivyoperator.LabelResourceSpecHash]
	if !ok {
		return fmt.Errorf("expected label %s not set", trivyoperator.LabelResourceSpecHash)
	}

	log = log.WithValues("kind", owner.GetObjectKind().GroupVersionKind().Kind,
		"name", owner.GetName(), "namespace", owner.GetNamespace(), "podSpecHash", podSpecHash)

	log.V(1).Info("Job complete")

	hasVulnReports := true
	containerImages, err := kube.GetContainerImagesFromJob(job, completedContainers...)
	if err != nil {
		return fmt.Errorf("getting container images: %w", err)
	}
	if r.Config.VulnerabilityScannerEnabled {
		hasVulnReports, err = hasVulnerabilityReports(ctx, r.VulnerabilityReadWriter, ownerRef, podSpecHash, containerImages)
		if err != nil {
			return err
		}
	}
	hasExposedSecretReports := true
	if r.Config.ExposedSecretScannerEnabled {
		hasExposedSecretReports, err = hasSecretReports(ctx, r.ExposedSecretReadWriter, ownerRef, podSpecHash, containerImages)
		if err != nil {
			return err
		}
	}
	if hasVulnReports && hasExposedSecretReports {
		log.V(1).Info("VulnerabilityReports already exist", "owner", owner)
		log.V(1).Info("Deleting complete scan job", "owner", owner)
		return r.deleteJob(ctx, job)
	}

	var vulnerabilityReports []v1alpha1.VulnerabilityReport
	var clusterVulnerabilityReports []v1alpha1.ClusterVulnerabilityReport
	var secretReports []v1alpha1.ExposedSecretReport
	var sbomNameSpacedReports []v1alpha1.SbomReport
	var sbomClusterReports []v1alpha1.ClusterSbomReport

	var merr error
	for containerName, containerImage := range containerImages {
		vulnReports, secReports, sbomReports, err := r.processScanJobResults(ctx, job, containerName, containerImage, owner)
		if err != nil {
			merr = multierr.Append(merr, err)
			continue
		}
		if vulnReports.vulnerabilityNamespaceReports != nil {
			vulnerabilityReports = append(vulnerabilityReports, *vulnReports.vulnerabilityNamespaceReports)
		}
		if vulnReports.vulnerabilityClusterReports != nil {
			clusterVulnerabilityReports = append(clusterVulnerabilityReports, *vulnReports.vulnerabilityClusterReports)
		}
		secretReports = append(secretReports, secReports...)
		if sbomReports != nil {
			sbomNameSpacedReports = append(sbomNameSpacedReports, sbomReports.sbomNamespaceReports...)
			sbomClusterReports = append(sbomClusterReports, sbomReports.sbomClusterReports...)
		}
	}
	if merr != nil {
		return merr
	}

	if r.Config.VulnerabilityScannerEnabled {
		if len(vulnerabilityReports) > 0 {
			err = r.VulnerabilityReadWriter.Write(ctx, vulnerabilityReports)
			if err != nil {
				return err
			}
		}
		if len(clusterVulnerabilityReports) > 0 {
			err = r.VulnerabilityReadWriter.WriteCluster(ctx, clusterVulnerabilityReports)
			if err != nil {
				return err
			}
		}
	}

	if r.Config.ExposedSecretScannerEnabled {
		err = r.ExposedSecretReadWriter.Write(ctx, secretReports)
		if err != nil {
			return err
		}
	}

	if r.Config.SbomGenerationEnable {
		err = r.SbomReadWriter.Write(ctx, sbomNameSpacedReports)
		if err != nil {
			return err
		}
	}
	if r.Config.ClusterSbomCacheEnable {
		if !r.Config.AltReportStorageEnabled || r.Config.AltReportDir == "" {
			err = r.SbomReadWriter.WriteCluster(ctx, sbomClusterReports)
			if err != nil {
				return err
			}
		}
	}

	log.V(1).Info("Deleting complete scan job", "owner", owner)
	return r.deleteJob(ctx, job)
}

type SbomReports struct {
	sbomNamespaceReports []v1alpha1.SbomReport
	sbomClusterReports   []v1alpha1.ClusterSbomReport
}

type VulnerabilityReports struct {
	vulnerabilityNamespaceReports *v1alpha1.VulnerabilityReport
	vulnerabilityClusterReports   *v1alpha1.ClusterVulnerabilityReport
}

func (r *ScanJobController) processScanJobResults(ctx context.Context,
	job *batchv1.Job,
	containerName,
	containerImage string,
	owner client.Object) (VulnerabilityReports, []v1alpha1.ExposedSecretReport, *SbomReports, error) {
	log := r.Logger.WithValues("job-results-processor", fmt.Sprintf("%s/%s", job.Namespace, job.Name))

	var vulnerabilityReports VulnerabilityReports
	var secretReports []v1alpha1.ExposedSecretReport
	sbomReports := &SbomReports{}

	podSpecHash, ok := job.Labels[trivyoperator.LabelResourceSpecHash]
	if !ok {
		return VulnerabilityReports{}, nil, nil, fmt.Errorf("expected label %s not set", trivyoperator.LabelResourceSpecHash)
	}

	logsStream, err := r.LogsReader.GetLogsByJobAndContainerName(ctx, job, containerName)
	if err != nil {
		if k8sapierror.IsNotFound(err) {
			log.V(1).Info("Cached job must have been deleted")
			return VulnerabilityReports{}, nil, nil, nil
		}
		if kube.IsPodControlledByJobNotFound(err) {
			log.V(1).Info("Pod must have been deleted")
			return VulnerabilityReports{}, nil, nil, r.deleteJob(ctx, job)
		}
		return VulnerabilityReports{}, nil, nil, fmt.Errorf("getting logs for pod %q: %w", job.Namespace+"/"+job.Name, err)
	}

	defer func() {
		err := logsStream.Close()
		if err != nil {
			log.V(1).Error(err, "could not close log stream")
		}
	}()

	vulnReportData, secretReportData, sbomReportData, err := r.Plugin.ParseReportData(r.PluginContext, containerImage, logsStream)
	if err != nil {
		return VulnerabilityReports{}, nil, nil, err
	}

	resourceLabelsToInclude := r.GetReportResourceLabels()
	additionalCustomLabels, err := r.GetAdditionalReportLabels()
	if err != nil {
		return VulnerabilityReports{}, nil, nil, err
	}
	if r.Config.VulnerabilityScannerEnabled {
		reportBuilder := vulnerabilityreport.NewReportBuilder(r.Client.Scheme()).
			Controller(owner).
			Container(containerName).
			Data(vulnReportData).
			PodSpecHash(podSpecHash).
			ResourceLabelsToInclude(resourceLabelsToInclude).
			AdditionalReportLabels(additionalCustomLabels)

		if r.Config.ScannerReportTTL != nil {
			reportBuilder.ReportTTL(r.Config.ScannerReportTTL)
		}

		report, clusterReport, err := reportBuilder.Get()
		if err != nil {
			return VulnerabilityReports{}, nil, nil, err
		}
		vulnerabilityReports = VulnerabilityReports{vulnerabilityNamespaceReports: report, vulnerabilityClusterReports: clusterReport}

		// Create or update ImageVulnerabilityReport for caching if cache is enabled
		if report != nil && r.ConfigData.VulnerabilityCacheEnabled() {
			log.V(1).Info("Vulnerability cache is enabled, attempting to create or update ImageVulnerabilityReport")
			imageDigest := vulnReportData.Artifact.Digest
			if imageDigest != "" {
				log.V(1).Info("Image digest found from scan data, using it for caching", "digest", imageDigest)
				ivarName := digestToResourceName(imageDigest)
				fullImageRef := vulnReportData.Artifact.Repository + ":" + vulnReportData.Artifact.Tag
				fullImageRefName := imageToResourceName(fullImageRef)
				// Check if the job's annotation indicates the original image lacked a digest
				if imageDigest != "" && job.Annotations != nil {
					if containerImages, ok := job.Annotations["trivy-operator.container-images"]; ok {
						var containerImagesMap map[string]string
						if err := json.Unmarshal([]byte(containerImages), &containerImagesMap); err == nil {
							if originalImage, exists := containerImagesMap[containerName]; exists && !strings.Contains(originalImage, "@") {
								ivarName = imageToResourceName(originalImage)
								log.V(1).Info("Original image lacked digest, using full original image reference for ImageVulnerabilityReport name", "container", containerName, "originalImage", originalImage, "name", ivarName)
							}
						}
					}
				} else if imageDigest == "" {
					ivarName = imageToResourceName(fullImageRef)
					log.V(1).Info("No digest available, using full image reference for ImageVulnerabilityReport name", "container", containerName, "image", fullImageRef, "name", ivarName)
				}
				// Compute a hash or truncated value for label-based indexing
				labelHash := computeHashForLabel(fullImageRefName)
				ivar := &v1alpha1.ImageVulnerabilityReport{
					ObjectMeta: metav1.ObjectMeta{
						Name: ivarName,
						Labels: map[string]string{
							"trivy-operator.cache.full-image-ref-hash": labelHash,
						},
						Annotations: map[string]string{
							"trivy-operator.aquasecurity.github.io/report-ttl": r.ConfigData.VulnerabilityCacheTTL(),
							"trivy-operator.cache.full-image-ref":              fullImageRef,
							"trivy-operator.cache.full-image-ref-name":         fullImageRefName,
						},
					},
					Report: vulnReportData,
				}
				// TODO: Increment cache update attempt metric
				err = r.Client.Create(ctx, ivar)
				if err != nil && k8sapierror.IsAlreadyExists(err) {
					log.V(1).Info("ImageVulnerabilityReport already exists, attempting update", "digest", imageDigest)
					existingIvar := &v1alpha1.ImageVulnerabilityReport{}
					err = r.Client.Get(ctx, client.ObjectKey{Name: ivar.Name}, existingIvar)
					if err == nil {
						ivar.ResourceVersion = existingIvar.ResourceVersion
						err = r.Client.Update(ctx, ivar, client.DryRunAll)
						if err == nil {
							err = r.Client.Update(ctx, ivar)
						}
					}
				}
				if err != nil {
					log.Error(err, "Failed to create or update ImageVulnerabilityReport", "digest", imageDigest)
					// TODO: Increment cache update failure metric
				} else {
					log.V(1).Info("Successfully cached ImageVulnerabilityReport", "digest", imageDigest)
					// TODO: Increment cache update success metric
				}
			} else {
				log.V(1).Info("No image digest found in scan data, skipping ImageVulnerabilityReport creation", "container", containerName, "image", containerImage)
			}
		} else if report != nil {
			// Diagnostic logging to check the state of ConfigData for vulnerability cache settings
			internalVal, internalOk := r.ConfigData["vulnerabilityCacheEnabled"]
			fallbackVal, fallbackOk := r.ConfigData["OPERATOR_VULNERABILITY_CACHE_ENABLED"]
			log.V(1).Info("Vulnerability cache check in ScanJobController", 
				"internalKey", "vulnerabilityCacheEnabled", "internalValue", internalVal, "internalExists", internalOk,
				"fallbackKey", "OPERATOR_VULNERABILITY_CACHE_ENABLED", "fallbackValue", fallbackVal, "fallbackExists", fallbackOk,
				"cacheEnabled", r.ConfigData.VulnerabilityCacheEnabled())
			log.V(1).Info("Vulnerability cache is disabled, skipping ImageVulnerabilityReport creation", "container", containerName, "image", containerImage)
		}
	}
	_, reused := job.Labels[trivyoperator.LabelReusedReport]
	if !ok {
		return VulnerabilityReports{}, nil, nil, fmt.Errorf("expected label %s not set", trivyoperator.LabelResourceSpecHash)
	}

	if r.ExposedSecretScannerEnabled && !reused {
		secretReport, err := exposedsecretreport.NewReportBuilder(r.Client.Scheme()).
			Controller(owner).
			Container(containerName).
			Data(secretReportData).
			PodSpecHash(podSpecHash).
			ResourceLabelsToInclude(resourceLabelsToInclude).
			AdditionalReportLabels(additionalCustomLabels).
			Get()
		if err != nil {
			return VulnerabilityReports{}, nil, nil, err
		}
		secretReports = append(secretReports, secretReport)
	}

	if r.SbomGenerationEnable && sbomReportData != nil && !reused {
		sbomReportBuilder := sbomreport.NewReportBuilder(r.Client.Scheme()).
			Controller(owner).
			Container(containerName).
			Data(*sbomReportData).
			PodSpecHash(podSpecHash).
			CacheTTL(r.Config.CacheReportTTL).
			ResourceLabelsToInclude(resourceLabelsToInclude).
			AdditionalReportLabels(additionalCustomLabels)
		sbomReport, clusterReport, err := sbomReportBuilder.Get()
		if err != nil {
			return VulnerabilityReports{}, nil, nil, err
		}
		if r.Config.ClusterSbomCacheEnable {
			sbomReports.sbomClusterReports = []v1alpha1.ClusterSbomReport{clusterReport}
		}
		sbomReports.sbomNamespaceReports = []v1alpha1.SbomReport{sbomReport}
	}
	if r.Config.AltReportStorageEnabled && r.Config.AltReportDir != "" {
		log.V(1).Info("Writing vulnerability reports to alternate storage", "dir", r.Config.AltReportDir)
		// Initilizing alternate writing directory var for trivy reports
		reportDir := r.Config.AltReportDir
		// Create subdirectories for each type of report
		clusterVulnerabilityDir := filepath.Join(reportDir, "cluster_vulnerability_reports")
		vulnerabilityDir := filepath.Join(reportDir, "vulnerability_reports")
		secretDir := filepath.Join(reportDir, "secret_reports")
		clusterSbomDir := filepath.Join(reportDir, "cluster_sbom_reports")
		sbomDir := filepath.Join(reportDir, "sbom_reports")

		// Ensure the directories exist
		for _, dir := range []string{vulnerabilityDir, secretDir, sbomDir, clusterSbomDir, clusterVulnerabilityDir} {
			if err := os.MkdirAll(dir, 0o750); err != nil {
				return VulnerabilityReports{}, []v1alpha1.ExposedSecretReport{}, &SbomReports{}, fmt.Errorf("failed to make directory %s: %w", dir, err)
			}
		}

		// Get workload kind and name
		workloadKind := owner.GetObjectKind().GroupVersionKind().Kind
		workloadName := owner.GetName()
		// Write cluster vulnerability reports to a file
		if vulnerabilityReports.vulnerabilityClusterReports != nil {
			reportData, err := json.Marshal(vulnerabilityReports.vulnerabilityClusterReports)
			if err != nil {
				return VulnerabilityReports{}, []v1alpha1.ExposedSecretReport{}, &SbomReports{}, fmt.Errorf("failed to marshal cluster vulnerability report: %w", err)
			}
			reportPath := filepath.Join(clusterVulnerabilityDir, fmt.Sprintf("%s-%s-%s.json", workloadKind, workloadName, containerName))
			err = os.WriteFile(reportPath, reportData, 0o600)
			if err != nil {
				return VulnerabilityReports{}, []v1alpha1.ExposedSecretReport{}, &SbomReports{}, fmt.Errorf("failed to write cluster vulnerability report: %w", err)
			}
		}

		// Write vulnerability reports to a file
		if vulnerabilityReports.vulnerabilityNamespaceReports != nil {
			reportData, err := json.Marshal(vulnerabilityReports.vulnerabilityNamespaceReports)
			if err != nil {
				return VulnerabilityReports{}, []v1alpha1.ExposedSecretReport{}, &SbomReports{}, fmt.Errorf("failed to marshal vulnerability report: %w", err)
			}
			reportPath := filepath.Join(vulnerabilityDir, fmt.Sprintf("%s-%s-%s.json", workloadKind, workloadName, containerName))
			err = os.WriteFile(reportPath, reportData, 0o600)
			if err != nil {
				return VulnerabilityReports{}, []v1alpha1.ExposedSecretReport{}, &SbomReports{}, fmt.Errorf("failed to join vulnerability report: %w", err)
			}
		}

		// Write secret reports to a file
		if len(secretReports) > 0 {
			reportData, err := json.Marshal(secretReports)
			if err != nil {
				return VulnerabilityReports{}, []v1alpha1.ExposedSecretReport{}, &SbomReports{}, fmt.Errorf("failed to marshal exposed secrets report: %w", err)
			}
			reportPath := filepath.Join(secretDir, fmt.Sprintf("%s-%s-%s.json", workloadKind, workloadName, containerName))
			err = os.WriteFile(reportPath, reportData, 0o600)
			if err != nil {
				return VulnerabilityReports{}, []v1alpha1.ExposedSecretReport{}, &SbomReports{}, fmt.Errorf("failed to join exposed secrets report: %w", err)
			}
		}

		// Write SBOM cluster reports to a file
		if len(sbomReports.sbomClusterReports) > 0 {
			reportData, err := json.Marshal(sbomReports.sbomClusterReports)
			if err != nil {
				return VulnerabilityReports{}, []v1alpha1.ExposedSecretReport{}, &SbomReports{}, fmt.Errorf("failed to marshal sbom cluster report: %w", err)
			}
			reportPath := filepath.Join(clusterSbomDir, fmt.Sprintf("%s-%s-%s.json", workloadKind, workloadName, containerName))
			err = os.WriteFile(reportPath, reportData, 0o600)
			if err != nil {
				return VulnerabilityReports{}, []v1alpha1.ExposedSecretReport{}, &SbomReports{}, fmt.Errorf("failed to join sbom cluster report: %w", err)
			}
		}

		// Write SBOM reports to a file
		if len(sbomReports.sbomNamespaceReports) > 0 {
			reportData, err := json.Marshal(sbomReports.sbomNamespaceReports)
			if err != nil {
				return VulnerabilityReports{}, []v1alpha1.ExposedSecretReport{}, &SbomReports{}, fmt.Errorf("failed to marshal sbom report: %w", err)
			}
			reportPath := filepath.Join(sbomDir, fmt.Sprintf("%s-%s-%s.json", workloadKind, workloadName, containerName))
			err = os.WriteFile(reportPath, reportData, 0o600)
			if err != nil {
				return VulnerabilityReports{}, []v1alpha1.ExposedSecretReport{}, &SbomReports{}, fmt.Errorf("failed to join sbom report: %w", err)
			}
		}
		// Return empty reports since alternate write method is used
		return VulnerabilityReports{}, []v1alpha1.ExposedSecretReport{}, &SbomReports{}, nil
	}
	return vulnerabilityReports, secretReports, sbomReports, nil
}

func digestToResourceName(d string) string {
	// sha256:<64hex>  -> sha256-<first 63>
	d = strings.ReplaceAll(d, ":", "-")
	if len(d) > 252 {
		return d[:252]
	}
	return d
}

// normalizeImageReference is a placeholder for potential future normalization logic.
// Currently, it returns the image reference as-is to avoid cache hit regressions.
func normalizeImageReference(image string) string {
	return image
}

// imageToResourceName converts a full image reference to a valid resource name by hashing if necessary.
// This ensures the name is compatible with Kubernetes resource naming constraints.
func imageToResourceName(image string) string {
	// Replace invalid characters for Kubernetes resource names
	image = strings.ReplaceAll(image, "/", "-")
	image = strings.ReplaceAll(image, ":", "-")
	image = strings.ReplaceAll(image, "@", "-")
	// Kubernetes resource names must be lowercase and no longer than 253 characters
	if len(image) > 252 {
		return fmt.Sprintf("img-%x", sha256.Sum256([]byte(image)))[:252]
	}
	return image
}

// computeHashForLabel creates a short hash of the input string for use in Kubernetes labels.
// Labels have stricter length and character constraints, so we ensure a fixed-length hash.
func computeHashForLabel(input string) string {
	hash := sha256.Sum256([]byte(input))
	return fmt.Sprintf("hash-%x", hash)[:63] // Kubernetes label values must be 63 chars or less
}

func (r *ScanJobController) completedContainers(ctx context.Context, scanJob *batchv1.Job) ([]string, error) {
	log := r.Logger.WithValues("job", fmt.Sprintf("%s/%s", scanJob.Namespace, scanJob.Name))

	statuses, err := r.GetTerminatedContainersStatusesByJob(ctx, scanJob)
	if err != nil {
		if k8sapierror.IsNotFound(err) {
			log.V(1).Info("Cached job must have been deleted")
			return []string{}, nil
		}
		if kube.IsPodControlledByJobNotFound(err) {
			log.V(1).Info("Pod must have been deleted")
			return []string{}, nil
		}
		return nil, err
	}

	// Retrieve the annotation to get the container image getting scanned
	containerImagesAnnotation, ok := scanJob.Annotations["trivy-operator.container-images"]
	if !ok {
		log.Error(nil, "Missing trivy-operator.container-images annotation")
		return nil, errors.New("missing trivy-operator.container-images annotation")
	}

	// Parse the JSON string into a map
	containerImages := make(map[string]string)
	err = json.Unmarshal([]byte(containerImagesAnnotation), &containerImages)
	if err != nil {
		log.Error(err, "Failed to parse trivy-operator.container-images annotation")
		return nil, fmt.Errorf("failed to parse trivy-operator.container-images annotation: %w", err)
	}

	completedContainers := make([]string, 0)
	for containerName, status := range statuses {
		if status.ExitCode == 0 {
			completedContainers = append(completedContainers, containerName)
			continue
		}

		// Get the container image for logging
		image, ok := containerImages[containerName]
		if !ok {
			image = "unknown"
		}

		if strings.Contains(status.Message, "no child with platform linux") {
			// Override the reason to be more descriptive for unsupported images
			status.Reason = "UnsupportedPlatform"
			log.Info("Scan job container",
				"container", containerName,
				"image", image,
				"status.reason", status.Reason,
				"status.message", "Container image is using an unsupported platform.")
		} else {
			log.Error(nil, "Scan job container",
				"container", containerName,
				"image", image,
				"status.reason", status.Reason,
				"status.message", status.Message)
		}
	}
	return completedContainers, nil
}

func (r *ScanJobController) deleteJob(ctx context.Context, job *batchv1.Job) error {
	err := r.Client.Delete(ctx, job, client.PropagationPolicy(metav1.DeletePropagationBackground))
	if err != nil {
		if k8sapierror.IsNotFound(err) {
			return nil
		}
		return fmt.Errorf("deleting job: %w", err)
	}
	return nil
}
